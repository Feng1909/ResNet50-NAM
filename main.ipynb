{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50-NAM: 一种新的注意力计算方式复现\n",
    "\n",
    "论文地址：https://arxiv.org/abs/2111.12419\n",
    "## 简介\n",
    "注意力机制在近年来大热，注意力机制可以帮助神经网络抑制通道中或者是空间中不太显著的特征。之前的很多的研究聚焦于如何通过注意力算子来获取显著性的特征。这些方法成功的发现了特征的不同维度之间的互信息量。但是，缺乏对权值的贡献因子的考虑，而这个贡献因子可以进一步的抑制不显著的特征。因此，我们瞄准了**利用权值的贡献因子来提升注意力**的效果。我们使用了**Batch Normalization**的缩放因子来表示权值的重要程度。这样可以**避免如SE，BAM和CBAM**一样增加全连接层和卷积层。这样，我们提出了一个新的注意力方式：基于归一化的注意力（**NAM**）。\n",
    "\n",
    "## 方法\n",
    "我们提出的NAM是一种轻量级的高效的注意力机制，我们采用了CBAM的模块集成方式，重新设计了通道注意力和空间注意力子模块，这样，NAM可以嵌入到每个网络block的最后。对于残差网络，可以嵌入到残差结构的最后。对于通道注意力子模块，我们使用了Batch Normalization中的缩放因子，如式子（1），缩放因子反映出各个通道的变化的大小，也表示了该通道的重要性。为什么这么说呢，可以这样理解，缩放因子即BN中的方差，方差越大表示该通道变化的越厉害，那么该通道中包含的信息会越丰富，重要性也越大，而那些变化不大的通道，信息单一，重要性小。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b397bce30f4e4662bb68b7c5a975393773e5df9b1f404b22840330e22c82fae8)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fbbd2c9dfc864689a2fb3a073eaa21bdcb9953ce89544162927185217e0aa2d8)\n",
    "\n",
    "其中$\\mu_B 和 \\sigma_B$为均值，$B$为标准差，$\\gamma 和 \\beta$是可训练的仿射变换参数（尺度和位移）[参考Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf).通道注意力子模块如图(1)和式(2)所示：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/86b757ab40f3460ba3430d7f7f71622d5d2c29622a5c4420921bfe033700b22b)\n",
    "其中$M_c$表示最后得到的输出特征，$\\gamma$是每个通道的缩放因子，因此，每个通道的权值可以通过 $W_\\gamma =\\gamma_i/\\sum_{j=0}\\gamma_j$ 得到。我们也使用一个缩放因子 $BN$ 来计算注意力权重，称为**像素归一化**。像素注意力如图(2)和式(3)所示：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2b24da52d2614ebca2c387564fd8920d93289777228d460b9a81b0a8c4df152a)\n",
    "\n",
    "为了抑制不重要的特征，作者在损失函数中加入了一个正则化项，如式(4)所示。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 数据集介绍：Cifar100\n",
    "链接：http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7baff8d84907478294ca778385fca688741d14db13d841c39f0125c41f0a0ac4)\n",
    "\n",
    "\n",
    "CIFAR100数据集有100个类。每个类有600张大小为32 × 32 32\\times 3232×32的彩色图像，其中500张作为训练集，100张作为测试集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码复现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.引入依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T13:20:42.826913Z",
     "iopub.status.busy": "2022-06-11T13:20:42.826067Z",
     "iopub.status.idle": "2022-06-11T13:20:42.831898Z",
     "shell.execute_reply": "2022-06-11T13:20:42.831257Z",
     "shell.execute_reply.started": "2022-06-11T13:20:42.826874Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.nn import functional as F\n",
    "\n",
    "from paddle.utils.download import get_weights_path_from_url\n",
    "import pickle\n",
    "import numpy as np\n",
    "from paddle import callbacks\n",
    "from paddle.vision.transforms import (\n",
    "    ToTensor, RandomHorizontalFlip, RandomResizedCrop, SaturationTransform, Compose,\n",
    "    HueTransform, BrightnessTransform, ContrastTransform, RandomCrop, Normalize, RandomRotation\n",
    ")\n",
    "from paddle.vision.datasets import Cifar100\n",
    "from paddle.io import DataLoader\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay, MultiStepDecay, LinearWarmup\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.定义NAM注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T13:20:44.676942Z",
     "iopub.status.busy": "2022-06-11T13:20:44.676055Z",
     "iopub.status.idle": "2022-06-11T13:20:44.683427Z",
     "shell.execute_reply": "2022-06-11T13:20:44.682782Z",
     "shell.execute_reply.started": "2022-06-11T13:20:44.676903Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Channel_Att(nn.Layer):\n",
    "    def __init__(self, channels=3, t=16):\n",
    "        super(Channel_Att, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.bn2 = nn.BatchNorm2D(self.channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.bn2(x)\n",
    "        weight_bn = self.bn2.weight.abs() / paddle.sum(self.bn2.weight.abs())\n",
    "        x = x.transpose([0, 2, 3, 1])\n",
    "        x = paddle.multiply(weight_bn, x)\n",
    "        x = x.transpose([0, 3, 1, 2])\n",
    "        x = F.sigmoid(x) * residual #\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Att(nn.Layer):\n",
    "    def __init__(self, channels=3, out_channels=None, no_spatial=True):\n",
    "        super(Att, self).__init__()\n",
    "        self.Channel_Att = Channel_Att(channels)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x_out1=self.Channel_Att(x)\n",
    "\n",
    "        return x_out1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.定义ResNet网络，加入NAM注意力机制\n",
    "本代码参考[Paddleclas](https://github.com/PaddlePaddle/PaddleClas)实现，代码中将分类类别设定为100类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T13:20:46.838025Z",
     "iopub.status.busy": "2022-06-11T13:20:46.837611Z",
     "iopub.status.idle": "2022-06-11T13:20:46.865809Z",
     "shell.execute_reply": "2022-06-11T13:20:46.865151Z",
     "shell.execute_reply.started": "2022-06-11T13:20:46.837987Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "__all__ = []\n",
    "model_urls = {\n",
    "    'resnet18': ('https://paddle-hapi.bj.bcebos.com/models/resnet18.pdparams',\n",
    "                 'cf548f46534aa3560945be4b95cd11c4'),\n",
    "    'resnet34': ('https://paddle-hapi.bj.bcebos.com/models/resnet34.pdparams',\n",
    "                 '8d2275cf8706028345f78ac0e1d31969'),\n",
    "    'resnet50': ('https://paddle-hapi.bj.bcebos.com/models/resnet50.pdparams',\n",
    "                 'ca6f485ee1ab0492d38f323885b0ad80'),\n",
    "    'resnet101': ('https://paddle-hapi.bj.bcebos.com/models/resnet101.pdparams',\n",
    "                  '02f35f034ca3858e1e54d4036443c92d'),\n",
    "    'resnet152': ('https://paddle-hapi.bj.bcebos.com/models/resnet152.pdparams',\n",
    "                  '7ad16a2f1e7333859ff986138630fd7a'),\n",
    "}\n",
    "\n",
    "class BasicBlock(nn.Layer):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 planes,\n",
    "                 stride=1,\n",
    "                 downsample=None,\n",
    "                 groups=1,\n",
    "                 base_width=64,\n",
    "                 dilation=1,\n",
    "                 norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2D\n",
    "\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\n",
    "                \"Dilation > 1 not supported in BasicBlock\")\n",
    "\n",
    "        self.conv1 = nn.Conv2D(\n",
    "            inplanes, planes, 3, padding=1, stride=stride, bias_attr=False)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2D(planes, planes, 3, padding=1, bias_attr=False)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.nam = Att(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = self.nam(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BottleneckBlock(nn.Layer):\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 planes,\n",
    "                 stride=1,\n",
    "                 downsample=None,\n",
    "                 groups=1,\n",
    "                 base_width=64,\n",
    "                 dilation=1,\n",
    "                 norm_layer=None):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2D\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        self.conv1 = nn.Conv2D(inplanes, width, 1, bias_attr=False)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = nn.Conv2D(\n",
    "            width,\n",
    "            width,\n",
    "            3,\n",
    "            padding=dilation,\n",
    "            stride=stride,\n",
    "            groups=groups,\n",
    "            dilation=dilation,\n",
    "            bias_attr=False)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = nn.Conv2D(\n",
    "            width, planes * self.expansion, 1, bias_attr=False)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.nam = Att(planes*4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out = self.nam(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Layer):\n",
    "    \"\"\"ResNet model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        Block (BasicBlock|BottleneckBlock): block module of model.\n",
    "        depth (int): layers of resnet, default: 50.\n",
    "        num_classes (int): output dim of last fc layer. If num_classes <=0, last fc layer\n",
    "                            will not be defined. Default: 1000.\n",
    "        with_pool (bool): use pool before the last fc layer or not. Default: True.\n",
    "\n",
    "    Examples:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from paddle.vision.models import ResNet\n",
    "            from paddle.vision.models.resnet import BottleneckBlock, BasicBlock\n",
    "\n",
    "            resnet50 = ResNet(BottleneckBlock, 50)\n",
    "\n",
    "            resnet18 = ResNet(BasicBlock, 18)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, depth, num_classes=100, with_pool=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        layer_cfg = {\n",
    "            18: [2, 2, 2, 2],\n",
    "            34: [3, 4, 6, 3],\n",
    "            50: [3, 4, 6, 3],\n",
    "            101: [3, 4, 23, 3],\n",
    "            152: [3, 8, 36, 3]\n",
    "        }\n",
    "        layers = layer_cfg[depth]\n",
    "        self.num_classes = num_classes\n",
    "        self.with_pool = with_pool\n",
    "        self._norm_layer = nn.BatchNorm2D\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.conv1 = nn.Conv2D(\n",
    "            3,\n",
    "            self.inplanes,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias_attr=False)\n",
    "        self.bn1 = self._norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        if with_pool:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "\n",
    "        if num_classes > 0:\n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2D(\n",
    "                    self.inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    1,\n",
    "                    stride=stride,\n",
    "                    bias_attr=False),\n",
    "                norm_layer(planes * block.expansion), )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes, planes, stride, downsample, 1, 64,\n",
    "                  previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.with_pool:\n",
    "            x = self.avgpool(x)\n",
    "\n",
    "        if self.num_classes > 0:\n",
    "            x = paddle.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def _resnet(arch, Block, depth, pretrained, **kwargs):\n",
    "    model = ResNet(Block, depth, **kwargs)\n",
    "    if pretrained:\n",
    "        assert arch in model_urls, \"{} model do not have a pretrained model now, you should set pretrained=False\".format(\n",
    "            arch)\n",
    "        weight_path = get_weights_path_from_url(model_urls[arch][0],\n",
    "                                                model_urls[arch][1])\n",
    "\n",
    "        param = paddle.load(weight_path)\n",
    "        model.set_dict(param)\n",
    "\n",
    "    return model\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"ResNet 50-layer model\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "\n",
    "    Examples:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from paddle.vision.models import resnet50\n",
    "\n",
    "            # build model\n",
    "            model = resnet50()\n",
    "\n",
    "            # build model and load imagenet pretrained weight\n",
    "            # model = resnet50(pretrained=True)\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', BottleneckBlock, 50, pretrained, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T13:36:07.769383Z",
     "iopub.status.busy": "2022-06-11T13:36:07.768489Z",
     "iopub.status.idle": "2022-06-11T13:36:07.906188Z",
     "shell.execute_reply": "2022-06-11T13:36:07.905648Z",
     "shell.execute_reply.started": "2022-06-11T13:36:07.769344Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    Layer (type)         Input Shape          Output Shape         Param #    \n",
      "================================================================================\n",
      "    Conv2D-1962       [[1, 3, 224, 224]]   [1, 64, 112, 112]        9,408     \n",
      "  BatchNorm2D-2458   [[1, 64, 112, 112]]   [1, 64, 112, 112]         256      \n",
      "      ReLU-630       [[1, 64, 112, 112]]   [1, 64, 112, 112]          0       \n",
      "    MaxPool2D-38     [[1, 64, 112, 112]]    [1, 64, 56, 56]           0       \n",
      "    Conv2D-1964       [[1, 64, 56, 56]]     [1, 64, 56, 56]         4,096     \n",
      "  BatchNorm2D-2460    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "      ReLU-631        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "    Conv2D-1965       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "  BatchNorm2D-2461    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "    Conv2D-1966       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \n",
      "  BatchNorm2D-2462    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      "    Conv2D-1963       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \n",
      "  BatchNorm2D-2459    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      "  BatchNorm2D-2463    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      "  Channel_Att-497     [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "      Att-497         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "BottleneckBlock-593   [[1, 64, 56, 56]]     [1, 256, 56, 56]          0       \n",
      "    Conv2D-1967       [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384     \n",
      "  BatchNorm2D-2464    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "      ReLU-632        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "    Conv2D-1968       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "  BatchNorm2D-2465    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "    Conv2D-1969       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \n",
      "  BatchNorm2D-2466    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      "  BatchNorm2D-2467    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      "  Channel_Att-498     [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "      Att-498         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "BottleneckBlock-594   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "    Conv2D-1970       [[1, 256, 56, 56]]    [1, 64, 56, 56]        16,384     \n",
      "  BatchNorm2D-2468    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "      ReLU-633        [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "    Conv2D-1971       [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,864     \n",
      "  BatchNorm2D-2469    [[1, 64, 56, 56]]     [1, 64, 56, 56]          256      \n",
      "    Conv2D-1972       [[1, 64, 56, 56]]     [1, 256, 56, 56]       16,384     \n",
      "  BatchNorm2D-2470    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      "  BatchNorm2D-2471    [[1, 256, 56, 56]]    [1, 256, 56, 56]        1,024     \n",
      "  Channel_Att-499     [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "      Att-499         [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "BottleneckBlock-595   [[1, 256, 56, 56]]    [1, 256, 56, 56]          0       \n",
      "    Conv2D-1974       [[1, 256, 56, 56]]    [1, 128, 56, 56]       32,768     \n",
      "  BatchNorm2D-2473    [[1, 128, 56, 56]]    [1, 128, 56, 56]         512      \n",
      "      ReLU-634        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "    Conv2D-1975       [[1, 128, 56, 56]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-2474    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "    Conv2D-1976       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \n",
      "  BatchNorm2D-2475    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "    Conv2D-1973       [[1, 256, 56, 56]]    [1, 512, 28, 28]       131,072    \n",
      "  BatchNorm2D-2472    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "  BatchNorm2D-2476    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "  Channel_Att-500     [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "      Att-500         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "BottleneckBlock-596   [[1, 256, 56, 56]]    [1, 512, 28, 28]          0       \n",
      "    Conv2D-1977       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \n",
      "  BatchNorm2D-2477    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "      ReLU-635        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "    Conv2D-1978       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-2478    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "    Conv2D-1979       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \n",
      "  BatchNorm2D-2479    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "  BatchNorm2D-2480    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "  Channel_Att-501     [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "      Att-501         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "BottleneckBlock-597   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "    Conv2D-1980       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \n",
      "  BatchNorm2D-2481    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "      ReLU-636        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "    Conv2D-1981       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-2482    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "    Conv2D-1982       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \n",
      "  BatchNorm2D-2483    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "  BatchNorm2D-2484    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "  Channel_Att-502     [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "      Att-502         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "BottleneckBlock-598   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "    Conv2D-1983       [[1, 512, 28, 28]]    [1, 128, 28, 28]       65,536     \n",
      "  BatchNorm2D-2485    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "      ReLU-637        [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "    Conv2D-1984       [[1, 128, 28, 28]]    [1, 128, 28, 28]       147,456    \n",
      "  BatchNorm2D-2486    [[1, 128, 28, 28]]    [1, 128, 28, 28]         512      \n",
      "    Conv2D-1985       [[1, 128, 28, 28]]    [1, 512, 28, 28]       65,536     \n",
      "  BatchNorm2D-2487    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "  BatchNorm2D-2488    [[1, 512, 28, 28]]    [1, 512, 28, 28]        2,048     \n",
      "  Channel_Att-503     [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "      Att-503         [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "BottleneckBlock-599   [[1, 512, 28, 28]]    [1, 512, 28, 28]          0       \n",
      "    Conv2D-1987       [[1, 512, 28, 28]]    [1, 256, 28, 28]       131,072    \n",
      "  BatchNorm2D-2490    [[1, 256, 28, 28]]    [1, 256, 28, 28]        1,024     \n",
      "      ReLU-638       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-1988       [[1, 256, 28, 28]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-2491    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "    Conv2D-1989       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-2492   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "    Conv2D-1986       [[1, 512, 28, 28]]   [1, 1024, 14, 14]       524,288    \n",
      "  BatchNorm2D-2489   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  BatchNorm2D-2493   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  Channel_Att-504    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "      Att-504        [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "BottleneckBlock-600   [[1, 512, 28, 28]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-1990      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-2494    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-639       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-1991       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-2495    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "    Conv2D-1992       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-2496   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  BatchNorm2D-2497   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  Channel_Att-505    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "      Att-505        [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "BottleneckBlock-601  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-1993      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-2498    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-640       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-1994       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-2499    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "    Conv2D-1995       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-2500   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  BatchNorm2D-2501   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  Channel_Att-506    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "      Att-506        [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "BottleneckBlock-602  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-1996      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-2502    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-641       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-1997       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-2503    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "    Conv2D-1998       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-2504   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  BatchNorm2D-2505   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  Channel_Att-507    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "      Att-507        [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "BottleneckBlock-603  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-1999      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-2506    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-642       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-2000       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-2507    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "    Conv2D-2001       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-2508   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  BatchNorm2D-2509   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  Channel_Att-508    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "      Att-508        [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "BottleneckBlock-604  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-2002      [[1, 1024, 14, 14]]    [1, 256, 14, 14]       262,144    \n",
      "  BatchNorm2D-2510    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "      ReLU-643       [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-2003       [[1, 256, 14, 14]]    [1, 256, 14, 14]       589,824    \n",
      "  BatchNorm2D-2511    [[1, 256, 14, 14]]    [1, 256, 14, 14]        1,024     \n",
      "    Conv2D-2004       [[1, 256, 14, 14]]   [1, 1024, 14, 14]       262,144    \n",
      "  BatchNorm2D-2512   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  BatchNorm2D-2513   [[1, 1024, 14, 14]]   [1, 1024, 14, 14]        4,096     \n",
      "  Channel_Att-509    [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "      Att-509        [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "BottleneckBlock-605  [[1, 1024, 14, 14]]   [1, 1024, 14, 14]          0       \n",
      "    Conv2D-2006      [[1, 1024, 14, 14]]    [1, 512, 14, 14]       524,288    \n",
      "  BatchNorm2D-2515    [[1, 512, 14, 14]]    [1, 512, 14, 14]        2,048     \n",
      "      ReLU-644        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "    Conv2D-2007       [[1, 512, 14, 14]]     [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-2516     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "    Conv2D-2008        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-2517    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "    Conv2D-2005      [[1, 1024, 14, 14]]    [1, 2048, 7, 7]       2,097,152   \n",
      "  BatchNorm2D-2514    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "  BatchNorm2D-2518    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "  Channel_Att-510     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "      Att-510         [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "BottleneckBlock-606  [[1, 1024, 14, 14]]    [1, 2048, 7, 7]           0       \n",
      "    Conv2D-2009       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-2519     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "      ReLU-645        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "    Conv2D-2010        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-2520     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "    Conv2D-2011        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-2521    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "  BatchNorm2D-2522    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "  Channel_Att-511     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "      Att-511         [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "BottleneckBlock-607   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "    Conv2D-2012       [[1, 2048, 7, 7]]      [1, 512, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-2523     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "      ReLU-646        [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "    Conv2D-2013        [[1, 512, 7, 7]]      [1, 512, 7, 7]       2,359,296   \n",
      "  BatchNorm2D-2524     [[1, 512, 7, 7]]      [1, 512, 7, 7]         2,048     \n",
      "    Conv2D-2014        [[1, 512, 7, 7]]     [1, 2048, 7, 7]       1,048,576   \n",
      "  BatchNorm2D-2525    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "  BatchNorm2D-2526    [[1, 2048, 7, 7]]     [1, 2048, 7, 7]         8,192     \n",
      "  Channel_Att-512     [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "      Att-512         [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "BottleneckBlock-608   [[1, 2048, 7, 7]]     [1, 2048, 7, 7]           0       \n",
      "AdaptiveAvgPool2D-38  [[1, 2048, 7, 7]]     [1, 2048, 1, 1]           0       \n",
      "     Linear-38           [[1, 2048]]            [1, 100]           204,900    \n",
      "================================================================================\n",
      "Total params: 23,826,468\n",
      "Trainable params: 23,659,812\n",
      "Non-trainable params: 166,656\n",
      "--------------------------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 387.81\n",
      "Params size (MB): 90.89\n",
      "Estimated Total Size (MB): 479.27\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 23826468, 'trainable_params': 23659812}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = resnet50()\n",
    "paddle.summary(net, (1,3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.自定义数据集处理方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T13:20:51.686605Z",
     "iopub.status.busy": "2022-06-11T13:20:51.686304Z",
     "iopub.status.idle": "2022-06-11T13:20:51.698011Z",
     "shell.execute_reply": "2022-06-11T13:20:51.697447Z",
     "shell.execute_reply.started": "2022-06-11T13:20:51.686575Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToArray(object):\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = np.transpose(img, [2, 0, 1])\n",
    "        img = img / 255.\n",
    "        return img.astype('float32')\n",
    "\n",
    "class RandomApply(object):\n",
    "    def __init__(self, transform, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.p < random.random():\n",
    "            return img\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "                                                                                                                    \n",
    "class LRSchedulerM(callbacks.LRScheduler):                                                                                                           \n",
    "    def __init__(self, by_step=False, by_epoch=True, warm_up=True):                                                                                                \n",
    "        super().__init__(by_step, by_epoch)                                                                                                                          \n",
    "        assert by_step ^ warm_up\n",
    "        self.warm_up = warm_up\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.by_epoch and not self.warm_up:\n",
    "            if self.model._optimizer and hasattr(\n",
    "                self.model._optimizer, '_learning_rate') and isinstance(\n",
    "                    self.model._optimizer._learning_rate, paddle.optimizer.lr.LRScheduler):                                                                                         \n",
    "                self.model._optimizer._learning_rate.step()                                                                                          \n",
    "                                                                                                                                                     \n",
    "    def on_train_batch_end(self, step, logs=None):                                                                                                   \n",
    "        if self.by_step or self.warm_up:                                                                                                                             \n",
    "            if self.model._optimizer and hasattr(\n",
    "                self.model._optimizer, '_learning_rate') and isinstance(\n",
    "                    self.model._optimizer._learning_rate, paddle.optimizer.lr.LRScheduler):                                                                                         \n",
    "                self.model._optimizer._learning_rate.step()\n",
    "            if self.model._optimizer._learning_rate.last_epoch >= self.model._optimizer._learning_rate.warmup_steps:\n",
    "                self.warm_up = False\n",
    "\n",
    "def _on_train_batch_end(self, step, logs=None):\n",
    "    logs = logs or {}\n",
    "    logs['lr'] = self.model._optimizer.get_lr()\n",
    "    self.train_step += 1\n",
    "    if self._is_write():\n",
    "        self._updates(logs, 'train')\n",
    "\n",
    "def _on_train_begin(self, logs=None):\n",
    "    self.epochs = self.params['epochs']\n",
    "    assert self.epochs\n",
    "    self.train_metrics = self.params['metrics'] + ['lr']\n",
    "    assert self.train_metrics\n",
    "    self._is_fit = True\n",
    "    self.train_step = 0\n",
    "\n",
    "callbacks.VisualDL.on_train_batch_end = _on_train_batch_end\n",
    "callbacks.VisualDL.on_train_begin = _on_train_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.在Cifar100数据集上训练模型\n",
    "使用Paddle自带的Cifar100数据集API加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = paddle.Model(resnet50())\n",
    "# 加载checkpoint\n",
    "# model.load('output/ResNet50-NAM/299.pdparams')\n",
    "MAX_EPOCH = 300\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 5e-4\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 128\n",
    "CIFAR_MEAN = [0.5071, 0.4865, 0.4409]\n",
    "CIFAR_STD = [0.1942, 0.1918, 0.1958]\n",
    "DATA_FILE = './data/data76994/cifar-100-python.tar.gz'\n",
    "\n",
    "model.prepare(\n",
    "    paddle.optimizer.Momentum(\n",
    "        learning_rate=LinearWarmup(CosineAnnealingDecay(LR, MAX_EPOCH), 2000, 0., LR),\n",
    "        momentum=MOMENTUM,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=WEIGHT_DECAY),\n",
    "    paddle.nn.CrossEntropyLoss(),\n",
    "    paddle.metric.Accuracy(topk=(1,5)))\n",
    "\n",
    "# 定义数据集增强方式\n",
    "transforms = Compose([\n",
    "    RandomCrop(32, padding=4),\n",
    "    RandomApply(BrightnessTransform(0.1)),\n",
    "    RandomApply(ContrastTransform(0.1)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(15),\n",
    "    ToArray(),\n",
    "    Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "])\n",
    "val_transforms = Compose([ToArray(), Normalize(CIFAR_MEAN, CIFAR_STD)])\n",
    "\n",
    "# 加载训练和测试数据集\n",
    "train_set = Cifar100(DATA_FILE, mode='train', transform=transforms)\n",
    "test_set = Cifar100(DATA_FILE, mode='test', transform=val_transforms)\n",
    "\n",
    "# 定义保存方式和训练可视化\n",
    "checkpoint_callback = paddle.callbacks.ModelCheckpoint(save_freq=1, save_dir='output/ResNet50-NAM')\n",
    "callbacks = [LRSchedulerM(),checkpoint_callback, callbacks.VisualDL('vis_logs/resnet50_nam.log')]\n",
    "\n",
    "# 训练模型\n",
    "model.fit(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    epochs=MAX_EPOCH, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    verbose=1, \n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.使用训练后的模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = paddle.Model(resnet50())\n",
    "models.load('output/ResNet50-NAM/1.pdparams')\n",
    "models.prepare()\n",
    "\n",
    "result = models.evaluate(test_set, verbose=1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  总结\n",
    "本次复现任务是一个很简单的小项目，是百度AI达人项目的一个课题，仅需要复现NAM的结构并对ResNet50进行微小的修改，这也是本人第一次对论文进行复现。本次复现主要参考了github上他人的Pytorch复现代码，并改写为PaddlePaddle框架代码，特别感谢百度李老师的指导！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
